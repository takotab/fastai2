{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.text.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.data\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data\n",
    "\n",
    "> Functions and transforms to help gather text data in a `Datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_vocab(count, min_freq=3, max_vocab=60000):\n",
    "    \"Create a vocab of `max_vocab` size from `Counter` `count` with items present more than `min_freq`\"\n",
    "    vocab = [o for o,c in count.most_common(max_vocab) if c >= min_freq]\n",
    "    for o in reversed(defaults.text_spec_tok): #Make sure all special tokens are in the vocab\n",
    "        if o in vocab: vocab.remove(o)\n",
    "        vocab.insert(0, o)\n",
    "    vocab = vocab[:max_vocab]\n",
    "    return vocab + [f'xxfake' for i in range(0, 8-len(vocab)%8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'd'])\n",
    "test_eq(set([x for x in make_vocab(count) if not x.startswith('xxfake')]), \n",
    "        set(defaults.text_spec_tok + 'a'.split()))\n",
    "test_eq(len(make_vocab(count))%8, 0)\n",
    "test_eq(set([x for x in make_vocab(count, min_freq=1) if not x.startswith('xxfake')]), \n",
    "        set(defaults.text_spec_tok + 'a b c d'.split()))\n",
    "test_eq(set([x for x in make_vocab(count,max_vocab=12, min_freq=1) if not x.startswith('xxfake')]), \n",
    "        set(defaults.text_spec_tok + 'a b c'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorText(TensorBase):   pass\n",
    "class LMTensorText(TensorText): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Numericalize(Transform):\n",
    "    \"Reversible transform of tokenized texts to numericalized ids\"\n",
    "    def __init__(self, vocab=None, min_freq=3, max_vocab=60000, sep=' '):\n",
    "        self.vocab,self.min_freq,self.max_vocab,self.sep = vocab,min_freq,max_vocab,sep\n",
    "        self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)})\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if dsets is None: return\n",
    "        if self.vocab is None:\n",
    "            count = dsets.counter if hasattr(dsets, 'counter') else Counter(p for o in dsets for p in o)\n",
    "            self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab)\n",
    "            self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n",
    "\n",
    "    def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n",
    "    def decodes(self, o): return TitledStr(self.sep.join([self.vocab[o_] for o_ in o if self.vocab[o_] != PAD]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize(min_freq=1, sep=' ')\n",
    "num.setup(L('This is an example of text'.split(), 'this is another text'.split()))\n",
    "test_eq(set([x for x in num.vocab if not x.startswith('xxfake')]), \n",
    "        set(defaults.text_spec_tok + 'This is an example of text this another'.split()))\n",
    "test_eq(len(num.vocab)%8, 0)\n",
    "start = 'This is an example of text'\n",
    "t = num(start.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t, tensor([11, 9, 12, 13, 14, 10]))\n",
    "test_eq(num.decode(t), start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize(min_freq=2, sep=' ')\n",
    "num.setup(L('This is an example of text'.split(), 'this is another text'.split()))\n",
    "test_eq(set([x for x in num.vocab if not x.startswith('xxfake')]), \n",
    "        set(defaults.text_spec_tok + 'is text'.split()))\n",
    "test_eq(len(num.vocab)%8, 0)\n",
    "t = num(start.split())\n",
    "test_eq(t, tensor([0, 9, 0, 0, 0, 10]))\n",
    "test_eq(num.decode(t), f'{UNK} is {UNK} {UNK} {UNK} text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'texts': ['This is an example of text', 'this is another text']})\n",
    "tl = TfmdLists(df, [attrgetter('text'), Tokenizer.from_df('texts'), Numericalize(min_freq=2, sep=' ')])\n",
    "test_eq(tl, [tensor([2, 8, 9, 10, 0, 0, 0, 11]), tensor([2, 9, 10,  0, 11])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM_DataLoader -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _maybe_first(o): return o[0] if isinstance(o, tuple) else o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_tokenizer(ds):\n",
    "    tok = getattr(ds, 'tokenizer', None)\n",
    "    if isinstance(tok, Tokenizer): return tok\n",
    "    if isinstance(tok, (list,L)):\n",
    "        for t in tok:\n",
    "            if isinstance(t, Tokenizer): return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_lengths(ds):\n",
    "    tok = _get_tokenizer(ds)\n",
    "    if tok is None: return\n",
    "    return tok.get_lengths(ds.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastcore.utils.ReindexCollection"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReindexCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: add backward\n",
    "@delegates()\n",
    "class LMDataLoader(TfmdDL):\n",
    "    def __init__(self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs):\n",
    "        self.items = ReindexCollection(dataset, cache=cache, tfm=_maybe_first)\n",
    "        self.seq_len = seq_len\n",
    "        if lens is None: lens = _get_lengths(dataset)\n",
    "        if lens is None: lens = [len(o) for o in self.items]\n",
    "        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\n",
    "        # The \"-1\" is to allow for final label, we throw away the end that's less than bs\n",
    "        corpus = round_multiple(sum(lens)-1, bs, round_down=True)\n",
    "        self.bl = corpus//bs #bl stands for batch length\n",
    "        self.n_batches = self.bl//(seq_len) + int(self.bl%seq_len!=0)\n",
    "        self.last_len = self.bl - (self.n_batches-1)*seq_len\n",
    "        self.make_chunks()\n",
    "        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\n",
    "        self.n = self.n_batches*bs\n",
    "\n",
    "    def make_chunks(self): self.chunks = Chunks(self.items, self.lens)\n",
    "    def shuffle_fn(self,idxs):\n",
    "        self.items.shuffle()\n",
    "        self.make_chunks()\n",
    "        return idxs\n",
    "\n",
    "    def create_item(self, seq):\n",
    "        if seq>=self.n: raise IndexError\n",
    "        sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len\n",
    "        st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len\n",
    "        txt = self.chunks[st : st+sl+1]\n",
    "        return LMTensorText(txt[:-1]),txt[1:]\n",
    "    \n",
    "    @delegates(TfmdDL.new)\n",
    "    def new(self, dataset=None, seq_len=72, **kwargs):\n",
    "        lens = self.lens.coll if dataset is None else None\n",
    "        return super().new(dataset=dataset, lens=lens, seq_len=seq_len, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs,sl = 4,3\n",
    "ints = L([0,1,2,3,4],[5,6,7,8,9,10],[11,12,13,14,15,16,17,18],[19,20],[21,22]).map(tensor)\n",
    "dl = LMDataLoader(ints, bs=bs, seq_len=sl)\n",
    "list(dl)\n",
    "test_eq(list(dl),\n",
    "    [[tensor([[0, 1, 2], [5, 6, 7], [10, 11, 12], [15, 16, 17]]),\n",
    "      tensor([[1, 2, 3], [6, 7, 8], [11, 12, 13], [16, 17, 18]])],\n",
    "     [tensor([[3, 4], [8,  9], [13, 14], [18, 19]]),\n",
    "      tensor([[4, 5], [9, 10], [14, 15], [19, 20]])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,sl = 4,3\n",
    "ints = L([0,1,2,3,4],[5,6,7,8,9,10],[11,12,13,14,15,16,17,18],[19,20],[21,22,23],[24]).map(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(ints, bs=bs, seq_len=sl)\n",
    "test_eq(list(dl),\n",
    "    [[tensor([[0, 1, 2], [6, 7, 8], [12, 13, 14], [18, 19, 20]]),\n",
    "      tensor([[1, 2, 3], [7, 8, 9], [13, 14, 15], [19, 20, 21]])],\n",
    "     [tensor([[3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]),\n",
    "      tensor([[4, 5, 6], [10, 11, 12], [16, 17, 18], [22, 23, 24]])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Check lens work\n",
    "dl = LMDataLoader(ints, lens=ints.map(len), bs=bs, seq_len=sl)\n",
    "test_eq(list(dl),\n",
    "    [[tensor([[0, 1, 2], [6, 7, 8], [12, 13, 14], [18, 19, 20]]),\n",
    "      tensor([[1, 2, 3], [7, 8, 9], [13, 14, 15], [19, 20, 21]])],\n",
    "     [tensor([[3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]),\n",
    "      tensor([[4, 5, 6], [10, 11, 12], [16, 17, 18], [22, 23, 24]])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(ints, bs=bs, seq_len=sl, shuffle=True)\n",
    "for x,y in dl: test_eq(x[:,1:], y[:,:-1])\n",
    "((x0,y0), (x1,y1)) = tuple(dl)\n",
    "#Second batch begins where first batch ended\n",
    "test_eq(y0[:,-1], x1[:,0]) \n",
    "test_eq(type(x0), LMTensorText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def truncate(self:TitledStr, n):\n",
    "    words = self.split(' ')[:n]\n",
    "    return TitledStr(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x: TensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x: LMTensorText, y, samples, ctxs=None, max_n=10, **kwargs):\n",
    "    return show_batch[TensorText](x, None, samples, ctxs=ctxs, max_n=max_n, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = ColSplitter()(df)\n",
    "tfms = [attrgetter('text'), Tokenizer.from_df('text'), Numericalize()]\n",
    "dsets = Datasets(df, [tfms], splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(bs=16, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i know that originally , this film was xxup not a box office hit , but in light of recent xxmaj hollywood releases ( most of which have been decidedly formula - ridden , plot less , pointless , \" save - the - blonde - chick - no - matter - what \" xxunk ) , xxmaj xxunk of xxmaj all xxmaj xxunk , certainly in this sorry context deserves</td>\n",
       "      <td>i know that originally , this film was xxup not a box office hit , but in light of recent xxmaj hollywood releases ( most of which have been decidedly formula - ridden , plot less , pointless , \" save - the - blonde - chick - no - matter - what \" xxunk ) , xxmaj xxunk of xxmaj all xxmaj xxunk , certainly in this sorry context deserves a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be written by a porn director and filled by porn actors , i would n't ever call them actors ! xxmaj the costumes seems to be stolen from a local school play . xxmaj its seems like a road movie with almost no monsters . xxmaj there is no fun at all in this piece of sh!t , only horror , but not in the way the director intended . i would</td>\n",
       "      <td>written by a porn director and filled by porn actors , i would n't ever call them actors ! xxmaj the costumes seems to be stolen from a local school play . xxmaj its seems like a road movie with almost no monsters . xxmaj there is no fun at all in this piece of sh!t , only horror , but not in the way the director intended . i would rather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxunk everyone else 's , and practically makes the movie by itself . xxmaj i 've always admired xxmaj keaton for his ability to deliver lines that feel improvised , no matter what script he 's following . xxmaj his character , xxmaj hunt xxmaj stevenson , is a likable , affable xxunk , a natural leader with a wise - ass xxunk . xxmaj but he has a fatal flaw common</td>\n",
       "      <td>everyone else 's , and practically makes the movie by itself . xxmaj i 've always admired xxmaj keaton for his ability to deliver lines that feel improvised , no matter what script he 's following . xxmaj his character , xxmaj hunt xxmaj stevenson , is a likable , affable xxunk , a natural leader with a wise - ass xxunk . xxmaj but he has a fatal flaw common to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxunk with a camera could be out done by a monkey . \\n\\n xxmaj the highlights ( what little there were ) came from the special effects , which were \" ok \" . xxmaj the acting for the most part was also \" ok \" ; though nothing special , it was of a higher quality than other b - movies i have seen in the past . \\n\\n xxmaj the</td>\n",
       "      <td>with a camera could be out done by a monkey . \\n\\n xxmaj the highlights ( what little there were ) came from the special effects , which were \" ok \" . xxmaj the acting for the most part was also \" ok \" ; though nothing special , it was of a higher quality than other b - movies i have seen in the past . \\n\\n xxmaj the rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to the police to prevent him from xxunk a \" valentine xxmaj day \" like massacre , with his gang members dressed as cops , of his rival mobsters . xxmaj while on trial xxmaj jimmy came to his senses and admitted his guilt willing to face the music and then , after his three year sentence is up , get his life back together . \\n\\n▁ xxrep 3 * xxup spoiler</td>\n",
       "      <td>the police to prevent him from xxunk a \" valentine xxmaj day \" like massacre , with his gang members dressed as cops , of his rival mobsters . xxmaj while on trial xxmaj jimmy came to his senses and admitted his guilt willing to face the music and then , after his three year sentence is up , get his life back together . \\n\\n▁ xxrep 3 * xxup spoiler xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>times painful to watch we see the dandy 's xxunk rise to fame ( thanks to that xxunk ad ! ) and the xxmaj jonestown xxunk fall from scene xxunk to bickering xxunk . xxmaj as the bands become more disjointed the friendships are stretched tension tight and at several points snap into xxunk and even on stage fights . xxmaj all of this is half funny and half tragic and believe</td>\n",
       "      <td>painful to watch we see the dandy 's xxunk rise to fame ( thanks to that xxunk ad ! ) and the xxmaj jonestown xxunk fall from scene xxunk to bickering xxunk . xxmaj as the bands become more disjointed the friendships are stretched tension tight and at several points snap into xxunk and even on stage fights . xxmaj all of this is half funny and half tragic and believe it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n",
    "test_eq(type(x), LMTensorText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(dls.valid_ds[0][0]), dls.valid.lens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_input(samples, pad_idx=1, pad_fields=0, pad_first=False, backwards=False):\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    pad_fields = L(pad_fields)\n",
    "    max_len_l = pad_fields.map(lambda f: max([len(s[f]) for s in samples]))\n",
    "    if backwards: pad_first = not pad_first\n",
    "    def _f(field_idx, x):\n",
    "        if field_idx not in pad_fields: return x\n",
    "        idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed\n",
    "        sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x))\n",
    "        pad =  x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx\n",
    "        x1 = torch.cat([pad, x] if pad_first else [x, pad])\n",
    "        if backwards: x1 = x1.flip(0)\n",
    "        return retain_type(x1, x)\n",
    "    return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(pad_input([(tensor([1,2,3]),1), (tensor([4,5]), 2), (tensor([6]), 3)], pad_idx=0), \n",
    "        [(tensor([1,2,3]),1), (tensor([4,5,0]),2), (tensor([6,0,0]), 3)])\n",
    "test_eq(pad_input([(tensor([1,2,3]), (tensor([6]))), (tensor([4,5]), tensor([4,5])), (tensor([6]), (tensor([1,2,3])))], pad_idx=0, pad_fields=1), \n",
    "        [(tensor([1,2,3]),(tensor([6,0,0]))), (tensor([4,5]),tensor([4,5,0])), ((tensor([6]),tensor([1, 2, 3])))])\n",
    "test_eq(pad_input([(tensor([1,2,3]),1), (tensor([4,5]), 2), (tensor([6]), 3)], pad_idx=0, pad_first=True), \n",
    "        [(tensor([1,2,3]),1), (tensor([0,4,5]),2), (tensor([0,0,6]), 3)])\n",
    "test_eq(pad_input([(tensor([1,2,3]),1), (tensor([4,5]), 2), (tensor([6]), 3)], pad_idx=0, backwards=True), \n",
    "        [(tensor([3,2,1]),1), (tensor([5,4,0]),2), (tensor([6,0,0]), 3)])\n",
    "x = test_eq(pad_input([(tensor([1,2,3]),1), (tensor([4,5]), 2), (tensor([6]), 3)], pad_idx=0, backwards=True), \n",
    "        [(tensor([3,2,1]),1), (tensor([5,4,0]),2), (tensor([6,0,0]), 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Check retain type\n",
    "x = [(TensorText([1,2,3]),1), (TensorText([4,5]), 2), (TensorText([6]), 3)]\n",
    "y = pad_input(x, pad_idx=0)\n",
    "for s in y: test_eq(type(s[0]), TensorText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_input_chunk(samples, pad_idx=1, pad_first=True, seq_len=72):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    def _f(x):\n",
    "        l = max_len - x.shape[0]\n",
    "        pad_chunk = x.new_zeros((l//seq_len) * seq_len) + pad_idx\n",
    "        pad_res   = x.new_zeros(l % seq_len) + pad_idx\n",
    "        x1 = torch.cat([pad_chunk, x, pad_res]) if pad_first else torch.cat([x, pad_res, pad_chunk])\n",
    "        return retain_type(x1, x)\n",
    "    return [(_f(s[0]), *s[1:]) for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(pad_input_chunk([(tensor([1,2,3,4,5,6]),1), (tensor([1,2,3]), 2), (tensor([1,2]), 3)], pad_idx=0, seq_len=2), \n",
    "        [(tensor([1,2,3,4,5,6]),1), (tensor([0,0,1,2,3,0]),2), (tensor([0,0,0,0,1,2]), 3)])\n",
    "test_eq(pad_input_chunk([(tensor([1,2,3,4,5,6]),), (tensor([1,2,3]),), (tensor([1,2]),)], pad_idx=0, seq_len=2), \n",
    "        [(tensor([1,2,3,4,5,6]),), (tensor([0,0,1,2,3,0]),), (tensor([0,0,0,0,1,2]),)])\n",
    "test_eq(pad_input_chunk([(tensor([1,2,3,4,5,6]),), (tensor([1,2,3]),), (tensor([1,2]),)], pad_idx=0, seq_len=2, pad_first=False), \n",
    "        [(tensor([1,2,3,4,5,6]),), (tensor([1,2,3,0,0,0]),), (tensor([1,2,0,0,0,0]),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _default_sort(x): return len(x[0])\n",
    "\n",
    "@delegates(TfmdDL)\n",
    "class SortedDL(TfmdDL):\n",
    "    def __init__(self, dataset, sort_func=None, res=None, **kwargs):\n",
    "        super().__init__(dataset, **kwargs)\n",
    "        self.sort_func = _default_sort if sort_func is None else sort_func\n",
    "        if res is None and self.sort_func == _default_sort: res = _get_lengths(dataset)\n",
    "        self.res = [self.sort_func(self.do_item(i)) for i in range_of(self.dataset)] if res is None else res\n",
    "        self.idx_max = np.argmax(self.res)\n",
    "\n",
    "    def get_idxs(self):\n",
    "        idxs = super().get_idxs()\n",
    "        if self.shuffle: return idxs\n",
    "        return sorted(idxs, key=lambda i: self.res[i], reverse=True)\n",
    "\n",
    "    def shuffle_fn(self,idxs):\n",
    "        idxs = np.random.permutation(len(self.dataset))\n",
    "        idx_max = np.where(idxs==self.idx_max)[0][0]\n",
    "        idxs[0],idxs[idx_max] = idxs[idx_max],idxs[0]\n",
    "        sz = self.bs*50\n",
    "        chunks = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n",
    "        chunks = [sorted(s, key=lambda i: self.res[i], reverse=True) for s in chunks]\n",
    "        sort_idx = np.concatenate(chunks)\n",
    "\n",
    "        sz = self.bs\n",
    "        batches = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n",
    "        sort_idx = np.concatenate(np.random.permutation(batches[1:-1])) if len(batches) > 2 else np.array([],dtype=np.int)\n",
    "        sort_idx = np.concatenate((batches[0], sort_idx) if len(batches)==1 else (batches[0], sort_idx, batches[-1]))\n",
    "        return iter(sort_idx)\n",
    "    \n",
    "    @delegates(TfmdDL.new)\n",
    "    def new(self, dataset=None, **kwargs):\n",
    "        res = self.res if dataset is None else None\n",
    "        return super().new(dataset=dataset, res=res, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [(tensor([1,2]),1), (tensor([3,4,5,6]),2), (tensor([7]),3), (tensor([8,9,10]),4)]\n",
    "dl = SortedDL(ds, bs=2, before_batch=partial(pad_input, pad_idx=0))\n",
    "test_eq(list(dl), [(tensor([[ 3,  4,  5,  6], [ 8,  9, 10,  0]]), tensor([2, 4])), \n",
    "                   (tensor([[1, 2], [7, 0]]), tensor([1, 3]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [(tensor(range(random.randint(1,10))),i) for i in range(101)]\n",
    "dl = SortedDL(ds, bs=2, create_batch=partial(pad_input, pad_idx=-1), shuffle=True, num_workers=0)\n",
    "batches = list(dl)\n",
    "max_len = len(batches[0][0])\n",
    "for b in batches: \n",
    "    assert(len(b[0])) <= max_len \n",
    "    test_ne(b[0][-1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = RandomSplitter()(range_of(df))\n",
    "dsets = Datasets(df, splits=splits, tfms=[tfms, [attrgetter(\"label\"), Categorize()]], dl_type=SortedDL)\n",
    "dls = dsets.dataloaders(before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformBlock for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextBlock(TransformBlock):\n",
    "    def __init__(self, tok_tfm, vocab=None, is_lm=False, seq_len=72):\n",
    "        return super().__init__(type_tfms=[tok_tfm, Numericalize(vocab)],\n",
    "                                dl_type=LMDataLoader if is_lm else SortedDL,\n",
    "                                dls_kwargs={} if is_lm else {'before_batch': partial(pad_input_chunk, seq_len=seq_len)})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(Tokenizer.from_df, keep=True)\n",
    "    def from_df(cls, text_cols, vocab=None, is_lm=False, seq_len=72, **kwargs):\n",
    "        return cls(Tokenizer.from_df(text_cols, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len)\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(Tokenizer.from_folder, keep=True)\n",
    "    def from_folder(cls, path, vocab=None, is_lm=False, seq_len=72, **kwargs):\n",
    "        return cls(Tokenizer.from_folder(path, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextDataLoaders -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextDataLoaders(DataLoaders):\n",
    "    @classmethod\n",
    "    @delegates(DataLoaders.from_dblock)\n",
    "    def from_folder(cls, path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, text_vocab=None, is_lm=False,\n",
    "                    tok_tfm=None, seq_len=72, **kwargs):\n",
    "        \"Create from imagenet style dataset in `path` with `train`,`valid`,`test` subfolders (or provide `valid_pct`).\"\n",
    "        splitter = GrandparentSplitter(train_name=train, valid_name=valid) if valid_pct is None else RandomSplitter(valid_pct, seed=seed)\n",
    "        blocks = [TextBlock.from_folder(path, text_vocab, is_lm, seq_len) if tok_tfm is None else TextBlock(tok_tfm, text_vocab, is_lm, seq_len)]\n",
    "        if not is_lm: blocks.append(CategoryBlock(vocab=vocab))\n",
    "        get_items = partial(get_text_files, folders=[train,valid]) if valid_pct is None else get_text_files\n",
    "        dblock = DataBlock(blocks=blocks,\n",
    "                           get_items=get_items,\n",
    "                           splitter=splitter,\n",
    "                           get_y=None if is_lm else parent_label)\n",
    "        return cls.from_dblock(dblock, path, path=path, seq_len=seq_len, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(DataLoaders.from_dblock)\n",
    "    def from_df(cls, df, path='.', valid_pct=0.2, seed=None, text_col=0, label_col=1, label_delim=None, y_block=None,\n",
    "                text_vocab=None, is_lm=False, valid_col=None, tok_tfm=None, seq_len=72, **kwargs):\n",
    "        blocks = [TextBlock.from_df(text_col, text_vocab, is_lm, seq_len) if tok_tfm is None else TextBlock(tok_tfm, text_vocab, is_lm, seq_len)]\n",
    "        if y_block is None and not is_lm:\n",
    "            blocks.append(MultiCategoryBlock if is_listy(label_col) and len(label_col) > 1 else CategoryBlock)\n",
    "        if y_block is not None and not is_lm: blocks += (y_block if is_listy(y_block) else [y_block])\n",
    "        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n",
    "        dblock = DataBlock(blocks=blocks,\n",
    "                           get_x=ColReader(text_col),\n",
    "                           get_y=None if is_lm else ColReader(label_col, label_delim=label_delim),\n",
    "                           splitter=splitter)\n",
    "        return cls.from_dblock(dblock, df, path=path, seq_len=seq_len, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, path, csv_fname='labels.csv', header='infer', delimiter=None, **kwargs):\n",
    "        df = pd.read_csv(Path(path)/csv_fname, header=header, delimiter=delimiter)\n",
    "        return cls.from_df(df, path=path, **kwargs)\n",
    "\n",
    "TextDataLoaders.from_csv = delegates(to=TextDataLoaders.from_df)(TextDataLoaders.from_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_torch_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 04_data.external.ipynb.\n",
      "Converted 05_data.transforms.ipynb.\n",
      "Converted 06_data.block.ipynb.\n",
      "Converted 07_vision.core.ipynb.\n",
      "Converted 08_vision.data.ipynb.\n",
      "Converted 09_vision.augment.ipynb.\n",
      "Converted 09b_vision.utils.ipynb.\n",
      "Converted 09c_vision.widgets.ipynb.\n",
      "Converted 10_tutorial.pets.ipynb.\n",
      "Converted 11_vision.models.xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 13a_metrics.ipynb.\n",
      "Converted 14_callback.schedule.ipynb.\n",
      "Converted 14a_callback.data.ipynb.\n",
      "Converted 15_callback.hook.ipynb.\n",
      "Converted 15a_vision.models.unet.ipynb.\n",
      "Converted 16_callback.progress.ipynb.\n",
      "Converted 17_callback.tracker.ipynb.\n",
      "Converted 18_callback.fp16.ipynb.\n",
      "Converted 19_callback.mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 20a_distributed.ipynb.\n",
      "Converted 21_vision.learner.ipynb.\n",
      "Converted 22_tutorial.imagenette.ipynb.\n",
      "Converted 23_tutorial.transfer_learning.ipynb.\n",
      "Converted 24_vision.gan.ipynb.\n",
      "Converted 30_text.core.ipynb.\n",
      "Converted 31_text.data.ipynb.\n",
      "Converted 32_text.models.awdlstm.ipynb.\n",
      "Converted 33_text.models.core.ipynb.\n",
      "Converted 34_callback.rnn.ipynb.\n",
      "Converted 35_tutorial.wikitext.ipynb.\n",
      "Converted 36_text.models.qrnn.ipynb.\n",
      "Converted 37_text.learner.ipynb.\n",
      "Converted 38_tutorial.ulmfit.ipynb.\n",
      "Converted 40_tabular.core.ipynb.\n",
      "Converted 41_tabular.data.ipynb.\n",
      "Converted 42_tabular.model.ipynb.\n",
      "Converted 43_tabular.learner.ipynb.\n",
      "Converted 45_collab.ipynb.\n",
      "Converted 50_datablock_examples.ipynb.\n",
      "Converted 60_medical.imaging.ipynb.\n",
      "Converted 65_medical.text.ipynb.\n",
      "Converted 70_callback.wandb.ipynb.\n",
      "Converted 71_callback.tensorboard.ipynb.\n",
      "Converted 72_callback.neptune.ipynb.\n",
      "Converted 97_test_utils.ipynb.\n",
      "Converted 99_pytorch_doc.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
