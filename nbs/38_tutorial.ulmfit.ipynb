{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning in text\n",
    "\n",
    "> How to fine-tune a language model and train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a pretrained Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get our data and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put it in a `Datasets`. For a language model, we don't have targets, so there is only one transform to numericalize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = ColSplitter()(df)\n",
    "tfms = [attrgetter(\"text\"), Tokenizer.from_df(\"text\"), Numericalize()]\n",
    "dsets = Datasets(df, [tfms], splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use that `Datasets` to create a `DataLoaders`. Here the class of `TfmdDL` we need to use is `LMDataLoader` which will concatenate all the texts in a source (with a shuffle at each epoch for the training set), split it in `bs` chunks then read continuously through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply with a factory method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, text_col='text', is_lm=True, valid_col='is_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this film is awful . xxmaj the xxup cgi is the very cheap gray blob xxup cgi . xxmaj the crocodile looks like a large gray xxunk . xxmaj the worst is that no effort at all is given to making it walk or look like it is alive . xxmaj it is mostly a photo - xxunk xxup cgi that is placed into scenes and you almost expect to</td>\n",
       "      <td>xxmaj this film is awful . xxmaj the xxup cgi is the very cheap gray blob xxup cgi . xxmaj the crocodile looks like a large gray xxunk . xxmaj the worst is that no effort at all is given to making it walk or look like it is alive . xxmaj it is mostly a photo - xxunk xxup cgi that is placed into scenes and you almost expect to see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" red \" ( cheech is another role ) is in town and wants to hang with him . xxmaj xxunk that he could kill two xxunk with one stone , xxmaj cheech xxunk xxmaj chong off and xxmaj red . xxmaj what kind of adventures will xxmaj chong and xxmaj red get into ? xxmaj will xxmaj cheech get his freak on ? xxmaj how long will xxmaj chong go without</td>\n",
       "      <td>red \" ( cheech is another role ) is in town and wants to hang with him . xxmaj xxunk that he could kill two xxunk with one stone , xxmaj cheech xxunk xxmaj chong off and xxmaj red . xxmaj what kind of adventures will xxmaj chong and xxmaj red get into ? xxmaj will xxmaj cheech get his freak on ? xxmaj how long will xxmaj chong go without some</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(dls, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.573556</td>\n",
       "      <td>4.047952</td>\n",
       "      <td>0.275492</td>\n",
       "      <td>57.280037</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze()\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.307861</td>\n",
       "      <td>4.162732</td>\n",
       "      <td>0.255753</td>\n",
       "      <td>64.246811</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.138291</td>\n",
       "      <td>4.076338</td>\n",
       "      <td>0.271093</td>\n",
       "      <td>58.929264</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.813226</td>\n",
       "      <td>4.041671</td>\n",
       "      <td>0.274977</td>\n",
       "      <td>56.921394</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.449531</td>\n",
       "      <td>4.080863</td>\n",
       "      <td>0.273898</td>\n",
       "      <td>59.196533</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned the pretrained language model to this corpus, we save the encoder since we will use it for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj with a title \" borrowed \" from xxmaj xxunk xxmaj xxunk and liberal xxunk of xxmaj kubrick , xxmaj xxunk and xxmaj xxunk it is painfully obvious that xxmaj thomas xxmaj clay considers himself a cut above the usual sort of rubbish our xxmaj british cinema xxunk out . \" robert xxmaj xxunk \" ( for short ) sets itself up as a realistic study of youthful alienation and at</td>\n",
       "      <td>xxmaj with a title \" borrowed \" from xxmaj xxunk xxmaj xxunk and liberal xxunk of xxmaj kubrick , xxmaj xxunk and xxmaj xxunk it is painfully obvious that xxmaj thomas xxmaj clay considers himself a cut above the usual sort of rubbish our xxmaj british cinema xxunk out . \" robert xxmaj xxunk \" ( for short ) sets itself up as a realistic study of youthful alienation and at the</td>\n",
       "      <td>xxmaj this xxmaj very that xxunk from from \" xxunk xxmaj xxunk 's \" xxmaj , \" xxunk 's i xxunk xxmaj xxmaj xxunk are are a xxunk that the xxunk xxmaj xxunk and this a \" - the xxunk xxunk of xxunk . xxmaj xxunk xxunk has . of xxmaj xxunk xxmaj xxunk \" is aka \" ) is out up as a xxunk and on the xxunk and xxunk the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of xxmaj oz \" ( that film 's makeup results though , worked xxunk , as this one 's does not ) . xxmaj and a comparison with xxmaj baryshnikov 's nutcracker in * his * production shows how wonderfully creative xxmaj baryshnikov 's nutcracker mask was - the \" jaws \" actually seemed to move whenever xxmaj baryshnikov xxunk his head back . \\n\\n xxmaj the dancing itself in the xxmaj</td>\n",
       "      <td>xxmaj oz \" ( that film 's makeup results though , worked xxunk , as this one 's does not ) . xxmaj and a comparison with xxmaj baryshnikov 's nutcracker in * his * production shows how wonderfully creative xxmaj baryshnikov 's nutcracker mask was - the \" jaws \" actually seemed to move whenever xxmaj baryshnikov xxunk his head back . \\n\\n xxmaj the dancing itself in the xxmaj macaulay</td>\n",
       "      <td>the xxunk 's xxunk xxunk 's is ) is are ) and out by and well is is xxunk ) work . xxmaj the the few of the xxunk 's xxunk is the * xxmaj xxunk list that xxmaj xxmaj and xxunk 's xxmaj is is . the xxunk xxunk \" scene made to be from he fuqua 's the head . to xxmaj xxmaj the xxunk is is this film xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>their boat driver is killed ( hung ) and the boat disappears , so they find themselves trapped and basically at the xxunk of the \" lady in xxmaj black . \" \\n\\n xxmaj so what can you expect to find here ? xxmaj plenty of xxunk ! xxmaj one of the characters has their lips sewn shut and is then hung upside down in the fireplace and accidentally slow - xxunk</td>\n",
       "      <td>boat driver is killed ( hung ) and the boat disappears , so they find themselves trapped and basically at the xxunk of the \" lady in xxmaj black . \" \\n\\n xxmaj so what can you expect to find here ? xxmaj plenty of xxunk ! xxmaj one of the characters has their lips sewn shut and is then hung upside down in the fireplace and accidentally slow - xxunk by</td>\n",
       "      <td>xxunk xxunk ( xxmaj . he up . the two is . but the go the in in trapped xxunk the beginning of the ship xxunk and the red \" \" xxmaj xxmaj the , is you say from see out is xxmaj the of fun , xxmaj the of the most is a own xxunk together , xxunk xxunk xxunk up down . the end . the xxunk down at by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxunk . i particularly enjoyed the ancient xxmaj xxunk class and the xxunk of ' xxunk ' . \\n\\n xxmaj it had a feeling of ' story of xxup o ' - that is , where people of means indulge in xxunk sexual adventure . xxmaj as she walks around the fantastic apartment in the buff , she is at ease - and why not , what is to xxunk a \"</td>\n",
       "      <td>. i particularly enjoyed the ancient xxmaj xxunk class and the xxunk of ' xxunk ' . \\n\\n xxmaj it had a feeling of ' story of xxup o ' - that is , where people of means indulge in xxunk sexual adventure . xxmaj as she walks around the fantastic apartment in the buff , she is at ease - and why not , what is to xxunk a \" devil</td>\n",
       "      <td>xxmaj xxmaj think enjoyed the xxunk xxmaj xxunk xxunk , the xxunk of the the ' . xxmaj xxmaj the 's a lot of xxunk xxunk ' xxmaj xxunk ' and a it , of the are the are themselves their their xxunk . xxmaj the a has in in xxunk xxunk , xxmaj xxmaj 's she is xxunk the , and xxunk is xxunk and is the do ? little xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect is because they could have taken it even further , but they did n't . \\n\\n xxmaj the mix of both is mixed . i thought it was funny , but as with most all comedies , it was n't xxup that funny . i had my mom and little sister watch it with me and the jokes we made about it were funnier than the jokes scripted . xxmaj there</td>\n",
       "      <td>is because they could have taken it even further , but they did n't . \\n\\n xxmaj the mix of both is mixed . i thought it was funny , but as with most all comedies , it was n't xxup that funny . i had my mom and little sister watch it with me and the jokes we made about it were funnier than the jokes scripted . xxmaj there were</td>\n",
       "      <td>xxmaj the of are have been the for more . but it did n't have xxmaj xxmaj the plot of xxunk xxunk a together xxmaj think it was a , but it i the other the , it was a a ok bad . xxmaj was to expectations xxunk i xxunk , it . me . i rest were got were the were very . the xxunk . . xxmaj the were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>there 's got to be a problem … xxbos xxmaj like xxmaj tarzan the xxmaj ape xxmaj man ( 1932 ) , only more so . xxmaj there 's more of everything , more animals , more varied xxmaj african tribes , and scenes in which the thought must be , if this was good with three or four lions , forty would be better . xxmaj tarzan xxunk with xxunk the</td>\n",
       "      <td>'s got to be a problem … xxbos xxmaj like xxmaj tarzan the xxmaj ape xxmaj man ( 1932 ) , only more so . xxmaj there 's more of everything , more animals , more varied xxmaj african tribes , and scenes in which the thought must be , if this was good with three or four lions , forty would be better . xxmaj tarzan xxunk with xxunk the crocodile</td>\n",
       "      <td>is no to be a lot with but xxmaj this many xxunk , xxmaj kid , hunter , xxunk ) , xxmaj two than - xxmaj the are no to a in it than , and of characters xxunk xxmaj , and a of which the characters of be xxunk but you is a . the xxunk four xxunk . then xxunk be xxunk than xxmaj the is a a , xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the most intriguing , and in my opinion , he deserved a bit more screen - time . xxmaj amy xxmaj ryan also performs her job xxunk . \\n\\n xxup before xxup the xxup devil xxup knows xxup you 're xxup dead is not an exceptional movie , but it proves that xxmaj lumet is still near the top of his game at the ( apparent ) twilight of an illustrious career</td>\n",
       "      <td>most intriguing , and in my opinion , he deserved a bit more screen - time . xxmaj amy xxmaj ryan also performs her job xxunk . \\n\\n xxup before xxup the xxup devil xxup knows xxup you 're xxup dead is not an exceptional movie , but it proves that xxmaj lumet is still near the top of his game at the ( apparent ) twilight of an illustrious career .</td>\n",
       "      <td>xxmaj interesting part the the the opinion , the was the xxmaj of . time time . \\n\\n the xxmaj xxunk is has a best with , xxmaj xxmaj xxunk xxup dawn xxup xxunk xxup begins xxup the xxup xxup not xxup a a xxunk film , but it 's that you xxunk xxmaj not a the top of the career . the time xxunk ) xxunk zone the old xxunk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>looks xxmaj american but , on the other hand , xxunk they prefer xxmaj american series ( xxrep 3 ! ) . xxmaj maybe it 's the language , or the spirit , but i think this series is more xxmaj english than xxmaj american . xxmaj by the way , the actors are really good and funny . xxmaj the acting is not superficial at all … xxbos xxmaj this film</td>\n",
       "      <td>xxmaj american but , on the other hand , xxunk they prefer xxmaj american series ( xxrep 3 ! ) . xxmaj maybe it 's the language , or the spirit , but i think this series is more xxmaj english than xxmaj american . xxmaj by the way , the actors are really good and funny . xxmaj the acting is not superficial at all … xxbos xxmaj this film is</td>\n",
       "      <td>like xxunk xxmaj the in the other hand , the is are the xxunk xxunk . especially 3 0 ) . xxmaj the it 's a only that but the xxunk of but it do it is is a of american - xxmaj english . xxmaj the the end , the series are all good . the . xxmaj the xxmaj is good good , all . xxmaj xxmaj this movie is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jacket , xxmaj pulp xxmaj fiction , xxmaj true xxmaj romance , 12 xxmaj monkeys , xxmaj xxunk , etc . xxmaj there 's no irony in watching good movies . xxmaj the true decline of the western civilization . xxmaj calling this a cult film is an xxmaj insult to true xxmaj cult classics like xxmaj xxunk man , or even xxmaj xxunk . xxmaj i 've said enough here .</td>\n",
       "      <td>, xxmaj pulp xxmaj fiction , xxmaj true xxmaj romance , 12 xxmaj monkeys , xxmaj xxunk , etc . xxmaj there 's no irony in watching good movies . xxmaj the true decline of the western civilization . xxmaj calling this a cult film is an xxmaj insult to true xxmaj cult classics like xxmaj xxunk man , or even xxmaj xxunk . xxmaj i 've said enough here . xxbos</td>\n",
       "      <td>and and xxunk xxmaj fiction , and xxunk xxmaj story , xxmaj xxmaj days , xxmaj xxunk xxmaj xxmaj . xxmaj the are no xxunk , this this movies . xxmaj the story story of the genre genre is xxmaj the the film movie movie is a xxunk american xxmaj the story science - . xxmaj the xxmaj xxmaj xxmaj xxmaj xxmaj xxunk xxmaj xxmaj the 'm seen that to that xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we need to use two set of transforms: one to numericalize the texts and the other to encode the labels as categories. Note that we have to use the same vocabulary as the one used in fine-tuning the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_vocab = dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = ColSplitter()(df)\n",
    "x_tfms = [attrgetter(\"text\"), Tokenizer.from_df(\"text\"), Numericalize(vocab=lm_vocab)]\n",
    "dsets = Datasets(df, splits=splits, tfms=[x_tfms, [attrgetter(\"label\"), Categorize()]], dl_type=SortedDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We once again use a subclass of `TfmdDL` for the dataloaders, since we want to sort the texts (sortish for the training set) by order of lengths. We also use `pad_collate` to create batches form texts of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(before_batch=pad_input_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is a factory method, once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, text_col=\"text\", text_vocab=lm_vocab, label_col='label', valid_col='is_valid', bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2, trunc_at=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we once again have a convenience function to create a classifier from this `DataLoaders` with the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, metrics=[accuracy], path=path,drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('enc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train with gradual unfreezing and differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.716138</td>\n",
       "      <td>0.586119</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.575132</td>\n",
       "      <td>0.463823</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.508140</td>\n",
       "      <td>0.474733</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478295</td>\n",
       "      <td>0.469056</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.379919</td>\n",
       "      <td>0.454718</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.392695</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.363343</td>\n",
       "      <td>0.470170</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319937</td>\n",
       "      <td>0.478629</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.284388</td>\n",
       "      <td>0.487241</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.256194</td>\n",
       "      <td>0.475965</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.239991</td>\n",
       "      <td>0.483625</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.229265</td>\n",
       "      <td>0.487338</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.opt = learn.create_opt()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj to review this movie , i without any doubt would have to quote that memorable scene in xxmaj tarantino 's \" pulp xxmaj fiction \" ( xxunk ) when xxmaj jules and xxmaj vincent are talking about xxmaj mia xxmaj wallace and what she does for a living . xxmaj jules tells xxmaj vincent that the \" only</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj how viewers react to this new \" adaption \" of xxmaj shirley xxmaj jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj the trouble with the book , \" memoirs of a xxmaj geisha \" is that it had xxmaj japanese xxunk but underneath the xxunk it was all an xxmaj american man 's way of thinking . xxmaj reading the book is like watching a magnificent ballet with great music , sets , and costumes yet performed by xxunk</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=4, trunc_at=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([8.0448e-05, 9.9992e-01]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"This was a good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.interpret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interp = Interpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probability</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj i 'm gon na xxunk the xxunk here a bit and say i enjoyed this . xxmaj however , the cartoon is really only going to appeal to those who have very xxunk xxunk . xxmaj it 's definitely something that most people will not get , as is the nature of xxunk . \\n\\n the animation is horrible , but yes , that 's the point . xxmaj the main character is foul mouthed , violent , and stupid . no redeeming qualities whatsoever . his wife xxunk and xxunk , apparently just barely capable of the most basic xxunk skills . most of these stories completely lack any kind of point . \\n\\n but again , that 's the point xxunk \\n\\n xxmaj if non xxunk , foul language , and complete and utter xxunk are your thing , you 're going to love this .</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.9995182752609253</td>\n",
       "      <td>7.638226509094238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj most italian horror lovers seem to hate this movie since because it has no connection to the first two xxmaj demons films . xxmaj and with the \" demons xxrep 3 i \" in the title , one would assume it would . xxmaj the problem is that this film was never intended to be part of the xxmaj demons series . xxmaj the distributors only a \" demons xxrep 3 i \" above its original title \" the xxmaj ogre \" to cash in on the other films popularity . xxmaj the new xxmaj american xxup dvd release of this picture has the title \" demons xxrep 3 i : xxmaj the xxmaj ogre \" on the box art but the film itself only says \" the xxmaj ogre \" . i do n't know if past releases had the title \" demons xxrep 3 i \"</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.9945324659347534</td>\n",
       "      <td>5.208930015563965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj for anyone who may not know what a one - actor movie was like , this is the best example . xxmaj this plot is ridiculous , and really makes no sense . xxmaj it 's full of xxunk situations , hackneyed lines , melodrama , comedy … you name it ! \\n\\n xxmaj but xxmaj xxunk xxmaj xxunk can make anything convincing , and this movie is by no means an exception . xxmaj everyone turns in a decent performance - xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk , xxmaj om xxmaj xxunk , xxmaj xxunk xxmaj xxunk … xxmaj but it is the xxmaj xxunk who xxunk everyone with his xxunk presence . xxmaj without him , this movie would have been a non - xxunk … xxmaj the story is about xxunk / mistaken identities / misunderstandings / love /</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.9913495779037476</td>\n",
       "      <td>4.750149726867676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj this one is a little better than the first one . xxmaj it still relies on a lot of its humor which basically keeps saying that the old xxmaj bond movies were not realistic . xxmaj that wears thin after so many xxunk . xxmaj the girls were more interesting in this one . \\n\\n xxmaj there is a tremendous amount of total gross out humor . xxmaj hopefully one day real comedy will come back .</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9783519506454468</td>\n",
       "      <td>3.832839250564575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj i 'm not sure under what circumstances director xxmaj visconti decided to film xxmaj james xxmaj cain 's novel \" the xxmaj xxunk xxmaj always xxmaj rings xxmaj twice \" ( i 'm not even sure if xxmaj xxunk acquired the book 's rights ) , but the resulting movie is definitely interesting . xxmaj it is not the best version of xxmaj cain 's story ( i like the 1981 version best ) , but thanks to xxmaj visconti 's excellent direction and the casting of xxmaj clara xxmaj xxunk and xxmaj xxunk xxmaj xxunk ( a very sensual couple ) , it is a must for noir fans . xxmaj visconti xxunk xxunk with noir xxunk to great effect . xxmaj the film is not perfect , though . xxmaj my main complaint is that the film is a little too long for its own good</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.9690021276473999</td>\n",
       "      <td>3.4738378524780273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj weaker entry in the xxmaj xxunk xxmaj drummond series , with xxmaj john xxmaj howard in the role . xxmaj usual funny xxunk and antics , but not much plot . xxmaj barrymore gets something to do as the inspector , xxunk xxunk to follow xxmaj drummond , xxmaj algy , and xxmaj xxunk on a wild xxunk chase ( mostly in circles ; perhaps the budget was tighter than usual ) to rescue poor xxmaj xxunk , who is being held captive by people who want to lure xxmaj drummond to his doom . xxmaj for those keeping score , in this one , xxmaj drummond is planning to ask xxmaj xxunk to marry him and xxmaj algy is worried about missing the baby 's xxunk . xxmaj it 's fun to see xxmaj algy and xxmaj xxunk dressed up as xxunk to blend in at xxmaj</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9453215003013611</td>\n",
       "      <td>2.9062843322753906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interp.plot_top_losses(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
